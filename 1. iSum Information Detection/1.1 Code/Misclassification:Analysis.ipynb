{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "project_path = '/content/drive/My Drive/Code Documentation Project/Issue Classification/'"
      ],
      "metadata": {
        "id": "mC5HYrrodI_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compare stand alone"
      ],
      "metadata": {
        "id": "MyJyQo0MJUmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty list to store DataFrames\n",
        "dfs = []\n",
        "\n",
        "for fold in range(1, 11):\n",
        "    #test_df = pd.read_csv(project_path + '/Ensemble Data/Literature/fold' + str(fold) + '.csv')\n",
        "    test_df = pd.read_csv(project_path + '/Ensemble Data/OpenJ9/fold' + str(fold) + '.csv')\n",
        "    dfs.append(test_df)\n",
        "\n",
        "# Concatenate all DataFrames in the list into one DataFrame\n",
        "issue_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "print(len(issue_df))\n",
        "print(issue_df.head())\n",
        "#issue_df.info()\n",
        "#issue_df[['Code','RF','LGBM','BERT','RoBERTa']]"
      ],
      "metadata": {
        "id": "DVNlLa-wdhYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame setup\n",
        "data = {\n",
        "    'Code': [1, 2, 3, 1, 2],  # Original labels\n",
        "    'LGBM_TFIDF': [1, 1, 3, 2, 1],  # Predictions from LightGBM\n",
        "    'RF': [1, 2, 2, 1, 3],  # Predictions from Random Forest\n",
        "    'BertCW': [1, 3, 3, 1, 2],  # Predictions from BERT\n",
        "    'RobertaCW': [2, 2, 3, 1, 2]  # Predictions from RoBERTa\n",
        "}\n",
        "\n",
        "#issue_df = pd.DataFrame(data)\n",
        "models=['RF','LGBM','BERT','RoBERTa']\n",
        "total_mis_all=[]\n",
        "def get_correction_stat(model):\n",
        "  misclassified = issue_df[issue_df[model] != issue_df['Code']]\n",
        "  total_mis=len(misclassified)\n",
        "  total_mis_all.append(total_mis)\n",
        "\n",
        "  correct=[]\n",
        "  for m in models:\n",
        "    if(m==model):\n",
        "      correct.append(0)\n",
        "      continue\n",
        "\n",
        "    corrected_by = (misclassified['Code'] == misclassified[m]).sum()/ total_mis\n",
        "    correct.append(round(corrected_by, 2))\n",
        "\n",
        "  return correct\n",
        "\n",
        "\n",
        "corr_df = pd.DataFrame(index=models, columns=models)\n",
        "for model in models:\n",
        "    corr_df.loc[model] = get_correction_stat(model)\n",
        "print(corr_df)\n"
      ],
      "metadata": {
        "id": "uhq0lR7QhQJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_at_least_correction_stat(model):\n",
        "    misclassified = issue_df[issue_df[model] != issue_df['Code']]\n",
        "    total_mis = len(misclassified)\n",
        "\n",
        "    corrected_by_at_least_one = 0\n",
        "    for idx, row in misclassified.iterrows():\n",
        "        other_models = [m for m in models if m != model]\n",
        "        if row['Code'] in [row[m] for m in other_models]:\n",
        "            corrected_by_at_least_one += 1\n",
        "\n",
        "    return corrected_by_at_least_one/total_mis\n",
        "\n",
        "# Create a DataFrame to store the correction stats\n",
        "corr_by_at_least_one_df = pd.DataFrame(index=models, columns=['Misclassifications Corrected by at Least One Model'])\n",
        "\n",
        "# Calculate and store the correction stats for each model\n",
        "for model in models:\n",
        "    corr_by_at_least_one_df.loc[model, 'Misclassifications Corrected by at Least One Model'] = get_at_least_correction_stat(model)\n",
        "\n",
        "# Print the DataFrame\n",
        "corr_by_at_least_one_df = corr_by_at_least_one_df.applymap(lambda x: round(x, 2))\n",
        "corr_df['>=1']=corr_by_at_least_one_df\n",
        "corr_df['wrong']=total_mis_all\n",
        "\n",
        "corr_df.to_csv(project_path + '/classification report/Misclassification/OpenJ9CorrectionIndidiualModels.csv',index=False)\n",
        "corr_df"
      ],
      "metadata": {
        "id": "3lunoMlsmM-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#compare ensemble"
      ],
      "metadata": {
        "id": "8q8yFGwGJWl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Function to calculate corrections\n",
        "def calculate_corrections(df, original, model_pred, correctors):\n",
        "    correction_counts = []\n",
        "    for label in np.unique(df[original]):\n",
        "        # Misclassified by the model\n",
        "        misclassified = df[(df[original] == label) & (df[model_pred] != label)]\n",
        "        misclassified_label_len=len(misclassified)\n",
        "        corrections = {}\n",
        "        for corrector in correctors:\n",
        "            # Corrected by each ensemble\n",
        "            corrected = misclassified[(misclassified[original] == misclassified[corrector])]\n",
        "            corrections[f\"Corrected by {corrector}\"] = len(corrected)/misclassified_label_len\n",
        "        correction_counts.append([label] + list(corrections.values()))\n",
        "    return pd.DataFrame(correction_counts, columns=['Label'] + list(corrections.keys()))\n",
        "\n",
        "# Calculate corrections\n",
        "correction_df = calculate_corrections(issue_df, 'Code', 'LGBM', ['Ens_LGBM', 'Ens_RF','Ens_BERT','Ens_RoBERTa'])\n",
        "\n",
        "correction_df['Label'] = correction_df['Label'].replace(label_mapping)\n",
        "\n",
        "#correction_df.to_csv(project_path + '/classification report/Misclassification/LiteratureCorrectionEnsemble.csv',index=False)\n",
        "correction_df.to_csv(project_path + '/classification report/Misclassification/OpenJ9CorrectionEnsemble.csv',index=False)\n",
        "correction_df\n"
      ],
      "metadata": {
        "id": "i-7PuPWGRVcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lm_VLxh8tWYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}