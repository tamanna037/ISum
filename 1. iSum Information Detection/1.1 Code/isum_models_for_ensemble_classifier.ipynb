{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sample of model preparation that are used in creating ensemble classifier. The parameters of lightGBM and weight of bert/roberta will be different for different datasets or tasks. Here we have provided a sample code. For the random forest, we used  model of Arya et al. "
      ],
      "metadata": {
        "id": "CwAZIxUhiovd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation of necessary libabries"
      ],
      "metadata": {
        "id": "9tt31UXMkpOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Em-RWyZAi5RU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqKwCM-gCBFj"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhpPGSR7UTBj"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "os.environ['CUDA_VISIBLE_DEVICES'] ='0' #gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datapath='Dataset.csv' #enter the correct path"
      ],
      "metadata": {
        "id": "nJTGEMeKlBR7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, Dataset\n",
        "import datasets\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold,StratifiedKFold"
      ],
      "metadata": {
        "id": "weyJTkg7SI-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizer & Model"
      ],
      "metadata": {
        "id": "wwTVbLyWlD84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce) or not\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\") #for bert, change to bert-base-uncased\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "def preprocess_function(examples, text_column_name = \"text\"):\n",
        "    return tokenizer(examples[text_column_name], truncation=True)\n",
        "\n",
        "ective. \n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=12) #for bert, change to bert-base-uncased and change num_labels for different dataset\n"
      ],
      "metadata": {
        "id": "fz7wXeHJSLaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GpQPVa1ilVIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "issue_df = pd.read_csv(datapath)\n",
        "issue_df = issue_df[['Text Content','Code']] #will only  consider text and label for this problem\n",
        "issue_df = issue_df.rename(columns={'Text Content': 'text', 'Code': 'label'}) #rename the columns\n",
        "print(issue_df.info())\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "label_encoder.fit(issue_df['label'])\n",
        "\n",
        "issue_df['label'] = label_encoder.transform(issue_df['label'])"
      ],
      "metadata": {
        "id": "cKsE-g-6SQ8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qy-KCB6IkE0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Testing of Bert/Roberta\n",
        "We have used cross fold validation where the dataset has been split into 10-folds using a stratified sampling. Each fold contains the data of each class in the same proportion and no two folds has any overlap between themselves\n"
      ],
      "metadata": {
        "id": "33CdSTe_mbiV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X0afDbnHWo6"
      },
      "outputs": [],
      "source": [
        "#cross-fold validation\n",
        "skf = StratifiedKFold(n_splits=10, random_state=10, shuffle=True)\n",
        "\n",
        "fold=0\n",
        "for trainval_index, test_index in skf.split(issue_df,issue_df['label']):  \n",
        "    fold=fold+1\n",
        "\n",
        "    trainval_df = issue_df.iloc[trainval_index]\n",
        "    test_df = issue_df.iloc[test_index]\n",
        "\n",
        "    #split for getting validation dataset\n",
        "    train_df, val_df = train_test_split(trainval_df, test_size=0.05, random_state=10, stratify=trainval_df['label'])\n",
        "\n",
        "    train_df = train_df[['text', 'label']]\n",
        "    val_df = val_df[['text', 'label']]\n",
        "    test_df = test_df[['text', 'label']]\n",
        "\n",
        "    #create dataset\n",
        "    train_dataset = Dataset.from_dict(train_df)\n",
        "    val_dataset =  Dataset.from_dict(val_df)\n",
        "    test_dataset = Dataset.from_dict(test_df)\n",
        "\n",
        "    issue_dataset = datasets.DatasetDict({\"train\":train_dataset,\"val\":val_dataset,\"test\":test_dataset})\n",
        "\n",
        "    #for class weight \n",
        "    count_0 = len(train_df[train_df['label'] == 0])\n",
        "    count_1 = len(train_df[train_df['label'] == 1])\n",
        "    count_2 = len(train_df[train_df['label'] == 2])\n",
        "    count_3 = len(train_df[train_df['label'] == 3])\n",
        "    count_4 = len(train_df[train_df['label'] == 4])\n",
        "    count_5 = len(train_df[train_df['label'] == 5])\n",
        "    count_6 = len(train_df[train_df['label'] == 6])\n",
        "    count_7 = len(train_df[train_df['label'] == 7])\n",
        "    count_8 = len(train_df[train_df['label'] == 8])\n",
        "    count_9 = len(train_df[train_df['label'] == 9])\n",
        "    count_10 = len(train_df[train_df['label'] == 10])\n",
        "    count_11 = len(train_df[train_df['label'] == 11])\n",
        "\n",
        "\n",
        "    class_weight_0 = (1 / count_0) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_1 = (1 / count_1) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_2 = (1 / count_2) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_3 = (1 / count_3) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_4 = (1 / count_4) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_5 = (1 / count_5) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_6 = (1 / count_6) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_7 = (1 / count_7) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_8 = (1 / count_8) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_9 = (1 / count_9) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_10 = (1 / count_10) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_11 = (1 / count_11) * (len(train_df) / len(set(train_df['label'])))\n",
        "\n",
        "    tokenized_issue_dataset = issue_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "    #trainer\n",
        "    class CustomTrainer(Trainer):\n",
        "        def compute_loss(self, model, inputs, return_outputs=False):\n",
        "            device = model.device\n",
        "            labels = inputs.get(\"labels\").to(device)\n",
        "            # forward pass\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.get(\"logits\").to(device)\n",
        "            # compute custom loss (suppose one has 3 labels with different weights)\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([class_weight_0,class_weight_1,class_weight_2,class_weight_3,class_weight_4,class_weight_5,class_weight_6,class_weight_7,class_weight_8,class_weight_9,class_weight_10,class_weight_11])).to(device)\n",
        "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "            return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16, #16\n",
        "        per_device_eval_batch_size=16, #16\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_issue_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_issue_dataset[\"val\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    #Training\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "    #Testing\n",
        "    \n",
        "    # Use the model to get predictions\n",
        "    test_predictions = trainer.predict(tokenized_issue_dataset[\"test\"])\n",
        "    # For each prediction, create the label with argmax\n",
        "    test_predictions_argmax = np.argmax(test_predictions[0], axis=1)\n",
        "\n",
        "    print(classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax))\n",
        "    report= classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "    report_df.to_csv('fold'+str(fold)+'.csv', index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jU1HE31xf_Z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Testing of LightGBM\n",
        "We have used cross fold validation where the dataset has been split into 10-folds using a stratified sampling. Each fold contains the data of each class in the same proportion and no two folds has any overlap between themselves"
      ],
      "metadata": {
        "id": "l3kwRaeIic-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import classification_report\n",
        "NUMBER_OF_CLASS=12\n",
        "\n",
        "issue_df = pd.read_csv(datapath)\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "label_encoder.fit(issue_df['Code'])\n",
        "issue_df['Code (Original)'] = issue_df['Code']\n",
        "issue_df['Code'] = label_encoder.transform(issue_df['Code'])\n",
        "\n",
        "#number_of_class\n",
        "class_id_map_df = issue_df.drop_duplicates(subset=['Code'])\n",
        "total_class=len(class_id_map_df)\n",
        "encoded_label_list = class_id_map_df['Code'].to_list()\n",
        "original_label_list = class_id_map_df['Code (Original)'].to_list()\n",
        "class_id_map = dict(zip(encoded_label_list, original_label_list))\n",
        "\n",
        "categ = ['aa','begauth','has_code','first_turn','last_turn']\n",
        "le = preprocessing.LabelEncoder()\n",
        "issue_df[categ] = issue_df[categ].apply(le.fit_transform)\n",
        "\n",
        "#cross-fold validation\n",
        "skf = StratifiedKFold(n_splits=10, random_state=10, shuffle=True)\n",
        "\n",
        "fold=0\n",
        "for train_index, test_index in skf.split(issue_df,issue_df['Code']):  \n",
        "    fold=fold+1\n",
        "    train_df = issue_df.iloc[train_index]\n",
        "    test_df = issue_df.iloc[test_index]\n",
        "    X_train = train_df[['len','tloc','cloc','tpos1','tpos2','clen','tlen','ppau','npau','aa','begauth','has_code','first_turn','last_turn']] #'Full Length',\n",
        "    y_train = train_df['Code']\n",
        "\n",
        "    X_test = test_df[['len','tloc','cloc','tpos1','tpos2','clen','tlen','ppau','npau','aa','begauth','has_code','first_turn','last_turn']] #'Full Length',\n",
        "    y_test = test_df['Code']\n",
        "\n",
        "\n",
        "    #change the parameters by searching on optuna for different cases \n",
        "    clf = lgb.LGBMClassifier(objective=\"multiclass\", class_weight='balanced', num_classes=13, metric='multi_logloss', feature_pre_filter= False,\n",
        "                             boosting_type=\"gbdt\", lambda_l1=5.6172275504022865e-05, lambda_l2= 0.10617995541784066,feature_fraction= 0.7,\n",
        "                             bagging_fraction= 1.0, bagging_freq= 0, min_child_samples= 20,num_leaves= 127)\n",
        "    \n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred=clf.predict(X_test)\n",
        "    label_list = []\n",
        "\n",
        "\n",
        "    for i in range(NUMBER_OF_CLASS):\n",
        "      label_list.append(class_id_map[i])\n",
        "\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "\n",
        "    for e in y_test:\n",
        "      true_labels.append(class_id_map[e])\n",
        "\n",
        "\n",
        "    for e in y_pred:\n",
        "      pred_labels.append(class_id_map[e])  \n",
        "    \n",
        "    print(classification_report(true_labels,pred_labels))"
      ],
      "metadata": {
        "id": "ofUqFvgNfzB_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}