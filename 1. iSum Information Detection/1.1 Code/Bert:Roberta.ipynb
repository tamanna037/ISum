{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FdFZYSWctHiE"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install accelerate>=0.21.0 -\n",
        "!pip install optuna\n",
        "!pip install optuna_integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UkRSUhX9rsp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] ='0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN8UWLiw3D56"
      },
      "outputs": [],
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "import logging\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jXcwxq_3WCb",
        "outputId": "65399e5e-b3ac-4c9a-a04d-76818d74d4f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPgjCVAw3D56"
      },
      "source": [
        "## **Read dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8CYLqgy3YlC"
      },
      "outputs": [],
      "source": [
        "project_path = '/content/drive/My Drive/Code Documentation Project/Issue Classification/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI-FkmZlj01Y"
      },
      "outputs": [],
      "source": [
        "lit_class_id_map= {0: 'Action on Issue',\n",
        " 1: 'Bug Reproduction',\n",
        " 2: 'Contribution and Commitment',\n",
        " 3: 'Expected Behaviour',\n",
        " 4: 'Investigation and Exploration',\n",
        " 5: 'Motivation',\n",
        " 6: 'Observed Bug Behaviour',\n",
        " 7: 'Potential New Issues and Requests',\n",
        " 8: 'Social Conversation',\n",
        " 9: 'Solution Discussion',\n",
        " 10: 'Task Progress',\n",
        " 11: 'Usage',\n",
        " 12: 'Workarounds'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXit5edUaDFu"
      },
      "outputs": [],
      "source": [
        "    test_predictions = trainer.predict(tokenized_issue_dataset[\"test\"])\n",
        "    # For each prediction, create the label with argmax\n",
        "    test_predictions_argmax = np.argmax(test_predictions[0], axis=1)\n",
        "\n",
        "    print(classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax))\n",
        "    report= classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    print(report_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGK4vG1JV6pp"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest values for the hyperparameters\n",
        "    learning_rate = trial.suggest_categorical('learning_rate', [1e-5, 2e-5,3e-5])\n",
        "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16])\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=1,\n",
        "        weight_decay=0.01,\n",
        "        report_to=\"none\"  # Avoid cluttering console outputs\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_issue_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_issue_dataset[\"val\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(eval_results)\n",
        "    # Optuna optimizes for a returned value, here we assume it's accuracy\n",
        "    return eval_results[\"eval_loss\"]\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=6)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_mh98jHY3h0"
      },
      "outputs": [],
      "source": [
        "    test_predictions = trainer.predict(tokenized_issue_dataset[\"test\"])\n",
        "    # For each prediction, create the label with argmax\n",
        "    test_predictions_argmax = np.argmax(test_predictions[0], axis=1)\n",
        "\n",
        "    print(classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax))\n",
        "    report= classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    print(report_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2t6C6dNflTgT"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "def objective(trial):\n",
        "    # Suggest values for the hyperparameters\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 2e-5, log=True)\n",
        "    per_device_train_batch_size = trial.suggest_categorical('per_device_train_batch_size', [8, 16])\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=0.01,\n",
        "        report_to=\"none\"  # Avoid cluttering console outputs\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_issue_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_issue_dataset[\"val\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Optuna optimizes for a returned value, here we assume it's accuracy\n",
        "    return eval_results[\"eval_accuracy\"]\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=20)\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcx32_YleYju"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch import nn\n",
        "from transformers import Trainer\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset, Dataset\n",
        "import datasets\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'learning_rate': [1e-5, 2e-5, 3e-5],\n",
        "    'per_device_train_batch_size': [8, 16],\n",
        "}\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") #roberta-base\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "def preprocess_function(examples, text_column_name = \"Text Content\"):\n",
        "    return tokenizer(examples[text_column_name], truncation=True)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=13) ##roberta-base\n",
        "\n",
        "\n",
        "for fold in range(10,11):\n",
        "    print(fold)\n",
        "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "\n",
        "    train_df=pd.read_csv(project_path+'/KFold/Literature/train_fold'+str(fold)+'.csv')\n",
        "    val_df=pd.read_csv(project_path+'/KFold/Literature/val_fold'+str(fold)+'.csv')\n",
        "    test_df=pd.read_csv(project_path+'/KFold/Literature/test_fold'+str(fold)+'.csv')\n",
        "\n",
        "    train_df = train_df[['Text Content', 'Code']]\n",
        "    val_df = val_df[['Text Content', 'Code']]\n",
        "    test_df = test_df[['Text Content', 'Code']]\n",
        "\n",
        "    train_df = train_df.rename(columns={'Code': 'label'})\n",
        "    val_df = val_df.rename(columns={'Code': 'label'})\n",
        "    test_df = test_df.rename(columns={'Code': 'label'})\n",
        "\n",
        "    train_dataset = Dataset.from_dict(train_df)\n",
        "    val_dataset =  Dataset.from_dict(val_df)\n",
        "    test_dataset = Dataset.from_dict(test_df)\n",
        "\n",
        "    issue_dataset = datasets.DatasetDict({\"train\":train_dataset,\"val\":val_dataset,\"test\":test_dataset})\n",
        "\n",
        "    count_0 = len(train_df[train_df['label'] == 0])\n",
        "    count_1 = len(train_df[train_df['label'] == 1])\n",
        "    count_2 = len(train_df[train_df['label'] == 2])\n",
        "    count_3 = len(train_df[train_df['label'] == 3])\n",
        "    count_4 = len(train_df[train_df['label'] == 4])\n",
        "    count_5 = len(train_df[train_df['label'] == 5])\n",
        "    count_6 = len(train_df[train_df['label'] == 6])\n",
        "    count_7 = len(train_df[train_df['label'] == 7])\n",
        "    count_8 = len(train_df[train_df['label'] == 8])\n",
        "    count_9 = len(train_df[train_df['label'] == 9])\n",
        "    count_10 = len(train_df[train_df['label'] == 10])\n",
        "    count_11 = len(train_df[train_df['label'] == 11])\n",
        "    count_12 = len(train_df[train_df['label'] == 12])\n",
        "\n",
        "\n",
        "    class_weight_0 = (1 / count_0) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_1 = (1 / count_1) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_2 = (1 / count_2) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_3 = (1 / count_3) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_4 = (1 / count_4) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_5 = (1 / count_5) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_6 = (1 / count_6) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_7 = (1 / count_7) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_8 = (1 / count_8) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_9 = (1 / count_9) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_10 = (1 / count_10) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_11 = (1 / count_11) * (len(train_df) / len(set(train_df['label'])))\n",
        "    class_weight_12 = (1 / count_12) * (len(train_df) / len(set(train_df['label'])))\n",
        "\n",
        "    tokenized_issue_dataset = issue_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "\n",
        "\n",
        "    class CustomTrainer(Trainer):\n",
        "        def compute_loss(self, model, inputs, return_outputs=False):\n",
        "            device = model.device\n",
        "            labels = inputs.get(\"labels\").to(device)\n",
        "            # forward pass\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            logits = outputs.get(\"logits\").to(device)\n",
        "            # compute custom loss (suppose one has 3 labels with different weights)\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor([class_weight_0,class_weight_1,class_weight_2,class_weight_3,class_weight_4,class_weight_5,class_weight_6,class_weight_7,class_weight_8,class_weight_9,class_weight_10,class_weight_11,class_weight_12])).to(device)\n",
        "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "            return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=32, #16\n",
        "        per_device_eval_batch_size=32, #16\n",
        "        num_train_epochs=10,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_issue_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_issue_dataset[\"val\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    break\n",
        "    trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "    # Use the model to get predictions\n",
        "    test_predictions = trainer.predict(tokenized_issue_dataset[\"test\"])\n",
        "    # For each prediction, create the label with argmax\n",
        "    test_predictions_argmax = np.argmax(test_predictions[0], axis=1)\n",
        "\n",
        "    print(classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax))\n",
        "    report= classification_report(np.array(test_df['label'].to_list()), test_predictions_argmax, output_dict=True)\n",
        "    report_df = pd.DataFrame(report).transpose()\n",
        "\n",
        "    print(report_df)\n",
        "    #report_df.to_csv(project_path+'classification report/Literature/R_CW/rcw_fold'+str(fold)+'.csv', index = False)\n",
        "\n",
        "    #pred_df=pd.read_csv(project_path+'/Ensemble Data/Literature/fold'+str(fold)+'.csv')\n",
        "    #pred_df['BERTCW']=test_predictions_argmax\n",
        "    #pred_df.to_csv(project_path+'/Ensemble Data/Literature/fold'+str(fold)+'.csv', index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}